{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Crime Data\n",
    "\n",
    "Final project for DS1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the crime report data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes2004_2018 = pd.read_csv(\"https://data.austintexas.gov/api/views/fdj4-gpfu/rows.csv?accessType=DOWNLOAD\", dtype={'Census Tract': object})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the columns - columns_largeset below is the simplification of the column i.e. replacing spaces with underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crimes2004_2018.columns)\n",
    "columns_largeset = ['inc_number', 'high_off_desc', 'high_off_code', 'fam_viol', 'date_time', 'date', 'time', 'report_date_time', 'report_date']\n",
    "columns2_largeset = ['report_time', 'loc_t ype', 'address', 'zip_code', 'council_district', 'apd_sector', 'apd_district', 'pra', 'cen_tract', 'clr_status', 'clr_date', 'ucr', 'cat_desc', 'x_coord', 'y_coord', 'latitude', 'longtitude', 'location']\n",
    "columns_largeset += columns2_largeset\n",
    "print(columns_largeset)\n",
    "crimes2004_2018.columns = columns_largeset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have resistence data for 2009 to 2016, so we will keep only the corresponding years of crime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list(crimes2004_2018['inc_number'].astype(str))\n",
    "temp = [l[0:4] for l  in temp]\n",
    "crimes2004_2018['year']= np.array(temp)\n",
    "crimes2009_2016 = crimes2004_2018.loc[crimes2004_2018['year'].isin(['2009','2010', '2011', '2012', '2013', '2014', '2015', '2016'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crimes2009_2016.shape)\n",
    "crimes2009_2016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the resistance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resistance_09 = pd.read_csv(\"https://data.austintexas.gov/api/views/sc8s-w4ka/rows.csv?accessType=DOWNLOAD\").astype(str)\n",
    "resistance_10 = pd.read_csv(\"https://data.austintexas.gov/api/views/q5ym-htjz/rows.csv?accessType=DOWNLOAD\").astype(str)\n",
    "resistance_11 = pd.read_csv(\"https://data.austintexas.gov/api/views/jipa-v8m5/rows.csv?accessType=DOWNLOAD\").astype(str)\n",
    "resistance_12 = pd.read_csv(\"https://data.austintexas.gov/api/views/bx9w-y5sd/rows.csv?accessType=DOWNLOAD\").astype(str)\n",
    "resistance_13 = pd.read_csv(\"https://data.austintexas.gov/api/views/qxx9-6iwk/rows.csv?accessType=DOWNLOAD\").astype(str)\n",
    "resistance_14 = pd.read_csv(\"https://data.austintexas.gov/api/views/vv43-e55n/rows.csv?accessType=DOWNLOAD\").astype(str)\n",
    "resistance_15 = pd.read_csv(\"https://data.austintexas.gov/api/views/iydp-s2cf/rows.csv?accessType=DOWNLOAD\").astype(str)\n",
    "resistance_16 = pd.read_csv(\"https://data.austintexas.gov/api/views/h8jq-pcz3/rows.csv?accessType=DOWNLOAD\").astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these datasets have different colnames - here, we standardize them so that we can combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_resistance = ['rin', 'prim_key', 'date', 'time', 'address', 'area_command', 'nature_of_contact', 'reason_desc', 'r2r_level', 'master_sub_id', 'sub_sex']\n",
    "columns2_resistance = ['sub_race', 'sub_ethn', 'sub_cond_desc', 'sub_res', 'weapon_1', 'weapon_2', 'weapon_3', 'weapon_4', 'weapon_5', 'num_shots', 'sub_eff', 'eff_on_officer']\n",
    "columns3_resistance = ['off_org_desc', 'off_comm_date', 'off_years_service', 'x_coord', 'y_coord', 'council_district']\n",
    "columns_resistance += columns2_resistance + columns3_resistance\n",
    "resistance_09.columns = columns_resistance\n",
    "resistance_10.columns = columns_resistance\n",
    "resistance_11.columns = columns_resistance\n",
    "resistance_12.columns = columns_resistance\n",
    "resistance_13.columns = columns_resistance\n",
    "resistance_14.columns = columns_resistance\n",
    "resistance_15.columns = columns_resistance\n",
    "resistance_16.columns = columns_resistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the data for all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resistances = [resistance_09,resistance_10,resistance_11,resistance_12, resistance_13,resistance_14,resistance_15, resistance_16]\n",
    "resistance = pd.concat(resistances,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resistance.shape)\n",
    "resistance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the resistance data with the crime data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the resistance data, each primary key (\"prim_key\") represents an incident, and each record is a single officer's report of that incident. This means that there are often multiple records per primary key. In order to merge one-to-one, we want only one record per primary key.\n",
    "\n",
    "First, we should determine which columns are at the report level and which are at the primary key level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique primary keys\n",
    "n_prim_key = len(resistance[\"prim_key\"].unique())\n",
    "\n",
    "# For each other column in the resistence dataset, check if the number of unique records in terms of the primary key i\n",
    "for col in resistance.columns[3:]:\n",
    "    n_col = len(resistance.groupby([\"prim_key\", col]))\n",
    "    if n_col != n_prim_key:\n",
    "        diff = n_col-n_prim_key\n",
    "        print(col, diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few of these variables that we might want to use as dependent variables, so we will clean them so that there is one value per primary key as follows:\n",
    "1. weapon_* --> a dummy marking whether any officer reported a weapon\n",
    "2. r2r_level -->  most severe r2r level (lower numbers = more severe resistance) reported by any officer\n",
    "3. number_of_shots --> dummy marking whether any officer reported a shot fired\n",
    "4. sub_eff --> whether any officer reported that a subject complained of pain/injury\n",
    "5. eff_on_officer --> whether any officer complained of pain/injury\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean weapon variables\n",
    "resistance[\"weapon\"] = 0\n",
    "for col in [\"weapon_1\", \"weapon_2\", \"weapon_3\", \"weapon_4\", \"weapon_5\"]:\n",
    "    resistance.loc[~resistance[col].isin([\"WEAPONLESS (PRESSURE POINTS/KICKS/HAND)\", \"-\", \"nan\"]), \"weapon\"] = 1 \n",
    "    # Note: nan is usually used to mark weapons 2-5, I think we can safely assume that it means no weapon\n",
    "# Show that we've correctly marked cases where any weapon was used\n",
    "#resistance[[\"weapon_1\", \"weapon_2\", \"weapon_3\", \"weapon_4\", \"weapon_5\", \"weapon\"]].head(20)\n",
    "#resistance.weapon.value_counts()\n",
    "# Mark the entire primary key as having a weapon used if any police officer reported it as such\n",
    "resistance[\"any_weapon\"] = resistance.groupby(['prim_key'])['weapon'].transform(max)\n",
    "\n",
    "# Clean r2r level \n",
    "resistance['r2r_level'] = resistance['r2r_level'].astype(float)\n",
    "resistance[\"max_r2r\"] = resistance.groupby(['prim_key'])['r2r_level'].transform(min) # Note: lower numbers = more severe resistance\n",
    "\n",
    "# Clean number of shots\n",
    "resistance[\"shot\"] = 0\n",
    "resistance.loc[resistance[\"num_shots\"]!=\"nan\", \"shot\"] = 1\n",
    "resistance[\"any_shot\"] = resistance.groupby(['prim_key'])['shot'].transform(max)\n",
    "\n",
    "# Clean subject complaint of pain/njury\n",
    "resistance[\"sub_complaint\"] = 0\n",
    "resistance.loc[~resistance[\"sub_eff\"].isin([\"NO COMPLAINT OF INJURY/PAIN\"]), \"sub_complaint\"] = 1\n",
    "resistance[\"any_sub_complaint\"] = resistance.groupby(['prim_key'])['sub_complaint'].transform(max)\n",
    "\n",
    "# Clean officer complaint of pain/njury\n",
    "resistance[\"off_complaint\"] = 0\n",
    "resistance.loc[~resistance[\"eff_on_officer\"].isin([\"NO COMPLAINT OF INJURY/PAIN\"]), \"off_complaint\"] = 1\n",
    "resistance[\"any_off_complaint\"] = resistance.groupby(['prim_key'])['off_complaint'].transform(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the columns of resistance that we want to use, and drop duplicates so that there is one observation per primary key instead of one obsercation per report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only necessary columns\n",
    "resistance = resistance[[\"prim_key\", \"date\", \"address\",\n",
    "                         \"any_weapon\", \"max_r2r\", \"any_shot\", \"any_sub_complaint\", \"any_off_complaint\"]]\n",
    "\n",
    "# Drop duplicates\n",
    "resistance = resistance.drop_duplicates()\n",
    "\n",
    "# Confirm data is now unique by primary key\n",
    "print(len(resistance[\"prim_key\"].unique()) == resistance.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"prim_key\" in the resistance data appears similar to \"inc_number\" in the crime data, but sometimes these have different numbers of characters. However, by looking closely at one year of data, we found that the addresses do tend to match exactly for cases that have similar \"prim_key\" and \"inc_number\" values that are slightly different lengths. \n",
    "\n",
    "We therefore take the first six characters of \"inc_number\" and \"prim_key\", and merge on these values PLUS the address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes2009_2016[\"inc_number\"] = crimes2009_2016[\"inc_number\"].apply(str)\n",
    "crimes2009_2016[\"prim_key_short\"] = crimes2009_2016[\"inc_number\"].str[0:7]\n",
    "resistance[\"prim_key_short\"] = resistance[\"prim_key\"].str[0:7]\n",
    "combined = pd.merge(crimes2009_2016, resistance, how=\"outer\", on=[\"prim_key_short\", \"address\"], indicator=True)\n",
    "combined._merge.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect there to be a lot of \"left_only\" cases in the crime data that are not in the resistance data - these are all the cases where there was a crime but no resistance. \n",
    "\n",
    "However, we should be concerned about \"right_only\" cases where there was resistence to a crime, but that crime was not recorded in the full dataset. There are about 1,603 of these cases (and 13,420 merged correctly)\n",
    "\n",
    "Below, we look at some cases that didn't merge correctly and then drop them so that we can run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.loc[combined._merge == \"right_only\", [\"prim_key\", \"date_y\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.loc[combined._merge != \"right_only\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll create a dummy marking the cases that were in the resistance data has having had some sort of resistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"any_resistance\"] = 0\n",
    "combined.loc[combined._merge == \"both\", \"any_resistance\"] = 1\n",
    "combined.drop(\"_merge\", axis=1, inplace=True)\n",
    "combined.any_resistance.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the outcome variables\n",
    "\n",
    "We use two target variables - one measuring whether there was any resistance and one measuring whether there was any resistance where anyone (officer or subject) complained of pain/injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking types\n",
    "combined.dtypes\n",
    "\n",
    "#replacing NaN with zero\n",
    "outcome = ['any_weapon','max_r2r','any_shot','any_sub_complaint','any_off_complaint']\n",
    "combined[outcome] = combined[outcome].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create target variables\n",
    "combined['Target1'] = combined['any_resistance']\n",
    "combined['Target2'] = combined[['any_sub_complaint','any_off_complaint']].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the Austin weather holidays; data set is from 2009-2016\n",
    "weather_data = pd.read_csv('Input Data/weather_09to16.csv')\n",
    "\n",
    "#re-format date from 'weather' to match the 'combined' DataFrame's date format\n",
    "weather_data['Date']= weather_data['Date'].str.replace('-', '/', regex=False)\n",
    "\n",
    "#merge in weather data\n",
    "combined = combined.join(weather_data.set_index('Date'), on='date_x')\n",
    "#combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare features based on when an incident occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge in US Holiday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the US holidays from csv; data set is from 1966-2020\n",
    "us_holidays = pd.read_csv('Input Data/usholidays.csv')\n",
    "\n",
    "#creating a new column for each holiday that will show True or False for each day if it falls on that holiday\n",
    "us_holidays['new_year'] = us_holidays['Holiday'] == \"New Year's Day\"\n",
    "us_holidays['mlk_day'] = us_holidays['Holiday'] == \"Birthday of Martin Luther King, Jr.\"\n",
    "us_holidays['wash_bday'] = us_holidays['Holiday'] == \"Washington's Birthday\"\n",
    "us_holidays['mem_day'] = us_holidays['Holiday'] == \"Memorial Day\"\n",
    "us_holidays['ind_day'] = us_holidays['Holiday'] == \"Independence Day\"\n",
    "us_holidays['labor_day'] = us_holidays['Holiday'] == \"Labor Day\"\n",
    "us_holidays['col_day'] = us_holidays['Holiday'] == \"Columbus Day\"\n",
    "us_holidays['vet_day'] = us_holidays['Holiday'] == \"Veterans Day\"\n",
    "us_holidays['thanksgiving'] = us_holidays['Holiday'] == \"Thanksgiving Day\"\n",
    "us_holidays['christmas'] = us_holidays['Holiday'] == \"Christmas Day\"\n",
    "\n",
    "#re-format date from 'us_holidays' to match the 'combined' DataFrame's date format\n",
    "us_holidays['Date']= us_holidays['Date'].str.replace('-', '/', regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the 'combined' and 'us_holidays' data frames on the date (date_x in 'combined', 'Date' in 'us_holidays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.join(us_holidays.set_index('Date'), on='date_x')\n",
    "\n",
    "#drop the unnecessary columns from the combined2 DataFrame (imported during join)\n",
    "combined.drop(['Unnamed: 0', 'Holiday'], axis = 1)\n",
    "\n",
    "#to 'fill' the combined  DataFrame, use slicing to select the columns from 'us_holidays' that need to be filled\n",
    "#fill combined DataFrame with 'False'\n",
    "#Essentially, we're saying that any day that doesn't match a US holiday should say 'False' in that columm\n",
    "j = list(us_holidays.columns)[3:]\n",
    "combined[j] = combined[j].fillna(value= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features based on date and time of the incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing to date format\n",
    "combined['date_time'] = combined['date_time'].astype('datetime64[ns]')\n",
    "#combined['report_date_time'] = combined['report_date_time'].astype('datetime64[ns]') # Arushi - I commented this out because it was taking forever to run on my computer\n",
    "#combined.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['year'] = combined['date_time'].year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the week\n",
    "day_of_week = []\n",
    "for i in range(combined.shape[0]):\n",
    "    day_of_week.append(combined['date_time'][i].weekday())\n",
    "combined['Day_of_Week'] = pd.DataFrame(day_of_week)\n",
    "#Here, 0 is Monday, 1 is Tuesday, 2 is Wednesday and so on.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in census data\n",
    "\n",
    "Census tract code for the two counties we care about should be six digits of the format 0ABCDE in order to merge with data from the Census API. We appear to have been something like the form BC.D, with leading/trailing zeroes dropped. Below, we clean the census tract data we were given to match the 0ABCDE format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split \n",
    "ct_split = combined[\"cen_tract\"].str.split(\".\", expand=True)\n",
    "combined[\"ct1\"] = ct_split[0]\n",
    "combined[\"ct2\"] = ct_split[1]\n",
    "\n",
    "# Calculate the length of each split piece - if the first is <4 digits, we add leading zeros, and if the second is <2 digits, we add trailing zeroes\n",
    "combined[\"ct1_len\"] = combined.ct1.str.len()\n",
    "combined[\"ct2_len\"] = combined.ct2.str.len()\n",
    "combined.loc[combined.ct2_len==1, \"ct2\"] = combined.loc[combined.ct2_len==1, \"ct2\"] + \"0\"\n",
    "combined.loc[combined.ct2_len.isnull() & (combined.ct1_len <=4), \"ct2\"] = \"00\"\n",
    "combined.loc[combined.ct1_len==1, \"ct1\"] = \"000\" + combined.loc[combined.ct1_len==1, \"ct1\"] \n",
    "combined.loc[combined.ct1_len==2, \"ct1\"] = \"00\" + combined.loc[combined.ct1_len==2, \"ct1\"] \n",
    "combined.loc[combined.ct1_len==3, \"ct1\"] = \"0\" + combined.loc[combined.ct1_len==3, \"ct1\"] \n",
    "\n",
    "combined[\"ct1_len\"] = combined.ct1.str.len()\n",
    "combined[\"ct2_len\"] = combined.ct2.str.len()\n",
    "\n",
    "# Examine cases where either piece is > 2 digits - should not be possible\n",
    "combined.loc[combined.ct1_len >4, \"cen_tract\"].value_counts()\n",
    "# These look like they're missing a decimal point. I'm just going to assume the last two characters go after the decimal point.\n",
    "# (It's only 11 cases anyway.)\n",
    "combined.loc[combined.ct1_len >4, \"ct1\"] = combined.loc[combined.ct1_len >4, \"cen_tract\"][:-2]\n",
    "combined.loc[combined.ct1_len >4, \"ct2\"] = combined.loc[combined.ct1_len >4, \"cen_tract\"][-2:]\n",
    "\n",
    "# Concatenate the two parts\n",
    "combined[\"tract\"] = combined.ct1 + combined.ct2\n",
    "\n",
    "combined.drop([\"cen_tract\",\"ct1\", \"ct1_len\", \"ct2\", \"ct2_len\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll pull some relevant indicators from the 2010 Census API. See full documentation of Table SF1 codes here: https://www.census.gov/prod/cen2010/doc/sf1.pdf.\n",
    "The indicators we are pulling are:\n",
    "- P0030001 - total population (which we will need for the denominator)\n",
    "- P0030002 - white population\n",
    "- P0030003 - black population\n",
    "- P0030004 - American Indian/native Alaskan population\n",
    "- P0030005 - Asian population \n",
    "- P0030006 - Hawaiian / Pacific Islander population\n",
    "- P0030007 - other race population\n",
    "- P0030008 - 2+ races population\n",
    "- P0040003 - Hispanic or Latino population (NOTE: not a race)\n",
    "- P0130001 - median age\n",
    "- P0130002 - median age for males\n",
    "- P0130003 - median age for females\n",
    "- P0180001 - total households (which we will need for the denominator)\n",
    "- P0200002 - households with any children under 18\n",
    "- P0250002 - households with any member over 65\n",
    "- P0190007 - households with husband-wife\n",
    "\n",
    "There are about 5723 cases that were missing census tract data in the original file. These (of course) did not merge with the census data from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"key\": \"7530e2501288a8dfb28803342f5d1493cf00cb96\",\n",
    "          \"state\": \"48\",   # Texas\n",
    "          \"county\": \"453,491\",  # Travis County, Williamson County\n",
    "          \"indicators\": \"P0030001,P0030002,P0030003,P0030004,P0030005,P0030006,P0030007,P0030008,P0040003,P0130001,P0130002,P0130003,P0180001,P0200002,P0250002,P0190007\"\n",
    "         }\n",
    "url = \"https://api.census.gov/data/2010/sf1?get=\"+params[\"indicators\"]+\"&for=tract:*&in=state:\"+params[\"state\"]+\"&in=county:\"+params[\"county\"]+\"&key=\"+params[\"key\"]\n",
    "response = requests.get(url, data = {'key':'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_data = pd.DataFrame(response.json()[1:])\n",
    "census_data.columns = response.json()[0]\n",
    "census_data.drop([\"state\", \"county\"], axis=1, inplace=True)\n",
    "combined = pd.merge(combined, census_data, how=\"outer\", on=\"tract\", indicator=True)\n",
    "combined._merge.value_counts()\n",
    "combined = combined.loc[combined._merge != \"right_only\", ]\n",
    "combined[\"has_census_data\"] = combined._merge == \"both\"\n",
    "combined.drop(\"_merge\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will calculate each indicator as percent of total population / percent of households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"P0030001\",\"P0030002\",\"P0030003\",\"P0030004\",\"P0030005\",\"P0030006\",\"P0030007\",\"P0030008\",\"P0040003\",\"P0130001\",\"P0130002\",\"P0130003\", \"P0180001\",\"P0200002\",\"P0250002\",\"P0190007\"]:\n",
    "    combined[col] = combined[col].astype(float)\n",
    "    \n",
    "for col in [\"P0030002\",\"P0030003\",\"P0030004\",\"P0030005\",\"P0030006\",\"P0030007\",\"P0030008\",\"P0040003\"]:\n",
    "    combined[col+\"_pct\"] = combined[col]/combined[\"P0030001\"]\n",
    "\n",
    "for col in [\"P0200002\",\"P0250002\",\"P0190007\"]:\n",
    "    combined[col+\"_pct\"] = combined[col]/combined[\"P0180001\"]\n",
    "    \n",
    "combined.drop( [\"P0030001\",\"P0030002\",\"P0030003\",\"P0030004\",\"P0030005\",\"P0030006\",\"P0030007\",\"P0030008\", \"P0040003\", \"P0180001\",\"P0200002\",\"P0250002\",\"P0190007\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will also pull some data from the American Community Survey (ACS) 5-year estimates. We need to use the 5-year estimates in order to have data down to the tract level. The full list of available variables is here: https://api.census.gov/data/2016/acs/acs5/variables.html. The indicators we're using are:\n",
    "- B19113_001E - median family income in the past 12 months\n",
    "- B01001_001E - total population (which we will need for the denominator)\n",
    "- B17001_001E - poverty status in the past 12 months \n",
    "- C18120_002e - total labor force (which we will need for the denominator)\n",
    "- C18120_003E - total employed in the labor force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"key\": \"7530e2501288a8dfb28803342f5d1493cf00cb96\",\n",
    "          \"state\": \"48\",   # Texas\n",
    "          \"county\": \"453,491\",  # Travis County, Williamson County\n",
    "          \"indicators\": \"B01001_001E,B19113_001E,B17001_001E,C18120_002E,C18120_003E\"\n",
    "         }\n",
    "url = \"https://api.census.gov/data/2016/acs/acs5?get=\"+params[\"indicators\"]+\"&for=tract:*&in=state:\"+params[\"state\"]+\"&in=county:\"+params[\"county\"]+\"&key=\"+params[\"key\"]\n",
    "response = requests.get(url, data = {'key':'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_data = pd.DataFrame(response.json()[1:])\n",
    "acs_data.columns = response.json()[0]\n",
    "acs_data.drop([\"state\", \"county\"], axis=1, inplace=True)\n",
    "for col in ['B01001_001E','B19113_001E','B17001_001E','C18120_002E','C18120_003E']:\n",
    "    acs_data[col] = acs_data[col].astype(float)\n",
    "#Missing values are recorded as -666666666; must clean these.\n",
    "acs_data.B19113_001E.replace(-666666666, None, inplace=True)\n",
    "combined = pd.merge(combined, acs_data, how=\"outer\", on=\"tract\", indicator=True)\n",
    "combined._merge.value_counts()\n",
    "combined = combined.loc[combined._merge != \"right_only\", ]\n",
    "combined.drop(\"_merge\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will calculate each indicator as percent of the relevant population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"B17001_001E_pct\"] = combined.B17001_001E/combined.B01001_001E\n",
    "combined[\"C18120_003E_pct\"] = combined.C18120_003E/combined.C18120_002E\n",
    "\n",
    "combined.drop(['B19113_001E','B17001_001E','C18120_002E','C18120_003E'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['inc_number', 'high_off_desc', 'high_off_code', 'fam_viol', 'date_time',\n",
       "       'date_x', 'time', 'report_date_time', 'report_date', 'report_time',\n",
       "       'loc_t ype', 'address', 'zip_code', 'council_district', 'apd_sector',\n",
       "       'apd_district', 'pra', 'clr_status', 'clr_date', 'ucr', 'cat_desc',\n",
       "       'x_coord', 'y_coord', 'latitude', 'longtitude', 'location', 'year',\n",
       "       'prim_key_short', 'prim_key', 'date_y', 'any_weapon', 'max_r2r',\n",
       "       'any_shot', 'any_sub_complaint', 'any_off_complaint', 'any_resistance',\n",
       "       'Target1', 'Target2', 'temp_max', 'temp_avg', 'temp_min', 'dew_max',\n",
       "       'dew_avg', 'dew_min', 'hum_max', 'hum_min', 'wind_max', 'wind_min',\n",
       "       'pres_max', 'pres_min', 'prec_avg', 'Unnamed: 0', 'Holiday', 'new_year',\n",
       "       'mlk_day', 'wash_bday', 'mem_day', 'ind_day', 'labor_day', 'col_day',\n",
       "       'vet_day', 'thanksgiving', 'christmas', 'tract', 'P0130001', 'P0130002',\n",
       "       'P0130003', 'has_census_data', 'P0030002_pct', 'P0030003_pct',\n",
       "       'P0030004_pct', 'P0030005_pct', 'P0030006_pct', 'P0030007_pct',\n",
       "       'P0030008_pct', 'P0040003_pct', 'P0200002_pct', 'P0250002_pct',\n",
       "       'P0190007_pct', 'B01001_001E', 'B17001_001E_pct', 'C18120_003E_pct'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
