{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection an Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn import svm \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brinaseidel/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>date_x</th>\n",
       "      <th>time</th>\n",
       "      <th>loc_t ype</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>council_district</th>\n",
       "      <th>apd_sector</th>\n",
       "      <th>apd_district</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longtitude</th>\n",
       "      <th>...</th>\n",
       "      <th>P0030006_pct</th>\n",
       "      <th>P0030007_pct</th>\n",
       "      <th>P0030008_pct</th>\n",
       "      <th>P0040003_pct</th>\n",
       "      <th>P0200002_pct</th>\n",
       "      <th>P0250002_pct</th>\n",
       "      <th>P0190007_pct</th>\n",
       "      <th>B19113_001E</th>\n",
       "      <th>B17001_001E_pct</th>\n",
       "      <th>C18120_003E_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-10-01 20:19:00</td>\n",
       "      <td>10/01/2014</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>RESIDENCE / HOME</td>\n",
       "      <td>78723.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>30.315506</td>\n",
       "      <td>-97.690678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.211769</td>\n",
       "      <td>0.064652</td>\n",
       "      <td>0.529424</td>\n",
       "      <td>0.402545</td>\n",
       "      <td>0.130133</td>\n",
       "      <td>0.337189</td>\n",
       "      <td>35600.0</td>\n",
       "      <td>0.994701</td>\n",
       "      <td>0.88417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-10-05 16:33:00</td>\n",
       "      <td>10/05/2014</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>RESIDENCE / HOME</td>\n",
       "      <td>78723.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>30.315506</td>\n",
       "      <td>-97.690678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.211769</td>\n",
       "      <td>0.064652</td>\n",
       "      <td>0.529424</td>\n",
       "      <td>0.402545</td>\n",
       "      <td>0.130133</td>\n",
       "      <td>0.337189</td>\n",
       "      <td>35600.0</td>\n",
       "      <td>0.994701</td>\n",
       "      <td>0.88417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-20 22:30:00</td>\n",
       "      <td>01/20/2012</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>PARKING LOTS / GARAGE</td>\n",
       "      <td>78723.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>30.320667</td>\n",
       "      <td>-97.687671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.211769</td>\n",
       "      <td>0.064652</td>\n",
       "      <td>0.529424</td>\n",
       "      <td>0.402545</td>\n",
       "      <td>0.130133</td>\n",
       "      <td>0.337189</td>\n",
       "      <td>35600.0</td>\n",
       "      <td>0.994701</td>\n",
       "      <td>0.88417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 20:00:00</td>\n",
       "      <td>01/01/2011</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>RESIDENCE / HOME</td>\n",
       "      <td>78723.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>30.320667</td>\n",
       "      <td>-97.687671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.211769</td>\n",
       "      <td>0.064652</td>\n",
       "      <td>0.529424</td>\n",
       "      <td>0.402545</td>\n",
       "      <td>0.130133</td>\n",
       "      <td>0.337189</td>\n",
       "      <td>35600.0</td>\n",
       "      <td>0.994701</td>\n",
       "      <td>0.88417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-03-06 05:00:00</td>\n",
       "      <td>03/06/2012</td>\n",
       "      <td>500.0</td>\n",
       "      <td>RESIDENCE / HOME</td>\n",
       "      <td>78723.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>30.320667</td>\n",
       "      <td>-97.687671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.211769</td>\n",
       "      <td>0.064652</td>\n",
       "      <td>0.529424</td>\n",
       "      <td>0.402545</td>\n",
       "      <td>0.130133</td>\n",
       "      <td>0.337189</td>\n",
       "      <td>35600.0</td>\n",
       "      <td>0.994701</td>\n",
       "      <td>0.88417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date_time      date_x    time              loc_t ype  zip_code  \\\n",
       "0  2014-10-01 20:19:00  10/01/2014  2019.0       RESIDENCE / HOME   78723.0   \n",
       "1  2014-10-05 16:33:00  10/05/2014  1633.0       RESIDENCE / HOME   78723.0   \n",
       "2  2012-01-20 22:30:00  01/20/2012  2230.0  PARKING LOTS / GARAGE   78723.0   \n",
       "3  2011-01-01 20:00:00  01/01/2011  2000.0       RESIDENCE / HOME   78723.0   \n",
       "4  2012-03-06 05:00:00  03/06/2012   500.0       RESIDENCE / HOME   78723.0   \n",
       "\n",
       "   council_district apd_sector apd_district   latitude  longtitude  \\\n",
       "0               1.0         ID            1  30.315506  -97.690678   \n",
       "1               1.0         ID            1  30.315506  -97.690678   \n",
       "2               1.0         ID            1  30.320667  -97.687671   \n",
       "3               1.0         ID            1  30.320667  -97.687671   \n",
       "4               1.0         ID            1  30.320667  -97.687671   \n",
       "\n",
       "        ...        P0030006_pct  P0030007_pct  P0030008_pct  P0040003_pct  \\\n",
       "0       ...              0.0004      0.211769      0.064652      0.529424   \n",
       "1       ...              0.0004      0.211769      0.064652      0.529424   \n",
       "2       ...              0.0004      0.211769      0.064652      0.529424   \n",
       "3       ...              0.0004      0.211769      0.064652      0.529424   \n",
       "4       ...              0.0004      0.211769      0.064652      0.529424   \n",
       "\n",
       "   P0200002_pct  P0250002_pct  P0190007_pct  B19113_001E  B17001_001E_pct  \\\n",
       "0      0.402545      0.130133      0.337189      35600.0         0.994701   \n",
       "1      0.402545      0.130133      0.337189      35600.0         0.994701   \n",
       "2      0.402545      0.130133      0.337189      35600.0         0.994701   \n",
       "3      0.402545      0.130133      0.337189      35600.0         0.994701   \n",
       "4      0.402545      0.130133      0.337189      35600.0         0.994701   \n",
       "\n",
       "   C18120_003E_pct  \n",
       "0          0.88417  \n",
       "1          0.88417  \n",
       "2          0.88417  \n",
       "3          0.88417  \n",
       "4          0.88417  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = pd.read_csv(\"../Personal Response_to_Resistance/Input Data/clean_data.csv\", header=0)\n",
    "#combined = pd.read_csv('clean_data.csv')\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APD sector\n",
    "apd_sector_dummies = pd.get_dummies(combined.apd_sector)\n",
    "cols = []\n",
    "for col in apd_sector_dummies.columns:\n",
    "    cols.append(\"apd_sector\"+str(col))\n",
    "apd_sector_dummies.columns = cols\n",
    "\n",
    "# Zip codes\n",
    "zip_dummies = pd.get_dummies(combined.zip_code)\n",
    "cols = []\n",
    "for col in zip_dummies.columns:\n",
    "    cols.append(\"zip\"+str(int(col)))\n",
    "zip_dummies.columns = cols\n",
    "\n",
    "combined = pd.concat([combined, apd_sector_dummies, zip_dummies], axis=1)\n",
    "\n",
    "# Location type\n",
    "loc_type_dummies = pd.get_dummies(combined['loc_t ype'])\n",
    "cols = []\n",
    "for col in loc_type_dummies.columns:\n",
    "    cols.append(\"loc_type\"+str(col))\n",
    "loc_type_dummies.columns = cols\n",
    "\n",
    "combined = pd.concat([combined, apd_sector_dummies, zip_dummies, loc_type_dummies], axis=1)\n",
    "\n",
    "combined.drop([\"month\", \"day_of_week\", \"hour_of_day\", \"tract\", \"loc_t ype\", \"apd_sector\", \"zip_code\", \"council_district\", \"latitude\", \"longtitude\", \"date_time\", \"date_x\", \"apd_district\", \"location\", \"year\", \"time\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases with missing that we will drop:\n",
      "0.0    0.993385\n",
      "1.0    0.006615\n",
      "Name: Target1, dtype: float64\n",
      "\n",
      " Nonmissing Cases:\n",
      "0.0    0.987127\n",
      "1.0    0.012873\n",
      "Name: Target1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Compare cases with missing data to cases with nonmissing data\n",
    "print(\"Cases with missing that we will drop:\")\n",
    "print(combined[combined.isnull().any(axis=1)].Target1.value_counts(normalize=True))\n",
    "print(\"\\n Nonmissing Cases:\")\n",
    "print(combined[~combined.isnull().any(axis=1)].Target1.value_counts(normalize=True))\n",
    "\n",
    "# Drop rows with missing data\n",
    "combined = combined.dropna()\n",
    "combined.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_outcomes = combined[[\"Target2\", \"any_weapon\", \"max_r2r\", \"any_shot\", \"any_sub_complaint\", \"any_off_complaint\", \"any_resistance\"]]\n",
    "combined.drop([\"any_weapon\", \"max_r2r\", \"any_shot\", \"any_sub_complaint\", \"any_off_complaint\", \"any_resistance\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "combined_scaled = pd.DataFrame(StandardScaler().fit_transform(combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_scaled.columns = combined.columns\n",
    "combined_scaled.Target1 = (combined_scaled[\"Target1\"]>0).astype(int)\n",
    "combined_scaled.Target2 = (combined_scaled[\"Target1\"]>1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined.drop([\"Target1\", \"Target2\"], axis=1)\n",
    "y = combined[\"Target1\"]\n",
    "\n",
    "# Regular dataset\n",
    "X_train, X_test, y_train, y_test, index_train, index_test = train_test_split(X, y, list(combined.index), test_size=0.2, random_state=100, stratify=y)\n",
    "\n",
    "# Scaled dataset\n",
    "X_train_scaled = combined_scaled.drop([\"Target1\", \"Target2\"], axis=1).iloc[index_train, ]\n",
    "y_train_scaled = combined.loc[index_train, \"Target1\"]\n",
    "X_test_scaled = combined_scaled.drop([\"Target1\", \"Target2\"], axis=1).iloc[index_test, ]\n",
    "y_test_scaled = combined.loc[index_test, \"Target1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.0    0.987127\n",
      "1.0    0.012873\n",
      "Name: Target1, dtype: float64\n",
      "\n",
      "Testing:\n",
      "0.0    0.987126\n",
      "1.0    0.012874\n",
      "Name: Target1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check that the stratification worked\n",
    "print(\"Training:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nTesting:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a baseline model\n",
    "\n",
    "Our baseline model uses only the information that was available to the police department before our data mining process. This includes:\n",
    "- Month dummies\n",
    "- Day of week dummies\n",
    "- Hour of day dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_sectors =  [col for col in combined if col.startswith('apd_sector') and col != \"apd_sector\"][:-1]\n",
    "months = [col for col in combined if col.startswith('month') and col != \"month\"][:-1]\n",
    "days = [col for col in combined if col.startswith('day') and col != \"day_of_week\"][:-1]\n",
    "hours = [col for col in combined if col.startswith('hour') and col != \"hour_of_day\"][:-1]\n",
    "baseline_cols = months + days + hours + apd_sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6250091424253844"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"entropy\").fit(X_train[baseline_cols], y_train)\n",
    "y_pred = model.predict_proba(X_test[baseline_cols])[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select appropriate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a logistic regression with a L1 penalty to return zeroes for some coefficients that are not useful\n",
    "l1_regularization = LogisticRegression(penalty='l1', solver='saga').fit(X_train_scaled, y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularization_coefs = dict(zip(X_train.columns, l1_regularization.coef_[0]))\n",
    "cols_to_use = [key for key, value in l1_regularization_coefs.items() if value != 0]\n",
    "print(\"Variables dropped: \" + str(len(X_train.columns) - len(cols_to_use)))\n",
    "print(\"Variables remaining: \" + str(len(cols_to_use)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show largest coefficients\n",
    "for key, value in l1_regularization_coefs.items():\n",
    "    l1_regularization_coefs[key] = abs(value)\n",
    "sorted(l1_regularization_coefs, key=l1_regularization_coefs.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up stratified k fold split\n",
    "skf = StratifiedKFold(n_splits=5, random_state=400)\n",
    "skf.get_n_splits(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression \n",
    "auc_lr = []\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    \n",
    "    X_train_fold, X_test_fold = X_train[cols_to_use].iloc[train_index], X_train[cols_to_use].iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    model = LogisticRegression(C=1e30).fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    y_pred_fold = model.predict_proba(X_test_fold)[:,1]\n",
    "    fpr1, tpr1, thresholds = roc_curve(y_test_fold, y_pred_fold)\n",
    "    auc_lr.append(auc(fpr1, tpr1))\n",
    "\n",
    "np.mean(auc_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive bayes\n",
    "auc_nb = []\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "        \n",
    "    X_train_fold, X_test_fold = X_train[cols_to_use].iloc[train_index], X_train[cols_to_use].iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    model = BernoulliNB().fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    y_pred_fold = model.predict_proba(X_test_fold)[:,1]\n",
    "    fpr2, tpr2, thresholds = roc_curve(y_test_fold, y_pred_fold)\n",
    "    auc_nb.append(auc(fpr2, tpr2))\n",
    "\n",
    "np.mean(auc_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "auc_dt = []\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    \n",
    "    X_train_fold, X_test_fold = X_train[cols_to_use].iloc[train_index], X_train[cols_to_use].iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\").fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    y_pred_fold = model.predict_proba(X_test_fold)[:,1]\n",
    "    fpr3, tpr3, thresholds = roc_curve(y_test_fold, y_pred_fold)\n",
    "    auc_dt.append(auc(fpr3, tpr3))\n",
    "\n",
    "np.mean(auc_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "auc_knn = []\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    \n",
    "    X_train_fold, X_test_fold = X_train[cols_to_use].iloc[train_index], X_train[cols_to_use].iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=5).fit(X_train_fold, y_train_fold)\n",
    "                                       \n",
    "    y_pred_fold = model.predict_proba(X_test_fold)[:,1]\n",
    "    fpr4, tpr4, thresholds = roc_curve(y_test_fold, y_pred_fold)\n",
    "    auc_knn.append(auc(fpr4, tpr4))\n",
    "\n",
    "np.mean(auc_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - THIS DOESN\"T WORK (in a reasonable amount of time)!\n",
    "#auc_svm = []\n",
    "#for train_index, test_index in skf.split(X_train, y_train):\n",
    "        \n",
    "#    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "#    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "#    svm_model = svm.SVC(kernel='linear')\n",
    "#    svm_model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "#    y_pred_fold = svm.predict_proba(X_test_fold)[:,1]\n",
    "#    fpr, tpr, thresholds = roc_curve(y_test_fold, y_pred_fold)\n",
    "#    auc_svm.append(auc(fpr, tpr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the four models that ran beyond the AUC values, we plot the ROC curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Receiver Operating Characteristic Curvers')\n",
    "plt.plot(fpr, tpr, 'b--', label = 'Logistic Regression (AUC=%0.2f)' % np.mean(auc_lr)) #logistic regression\n",
    "plt.plot(fpr2, tpr2, 'g--', label = 'Naive Bayes (AUC=%0.2f)' % np.mean(auc_nb)) #naive bayes\n",
    "plt.plot(fpr3, tpr3, 'm--', label = 'Decision Tree (AUC=%0.2f)' % np.mean(auc_dt)) #decision tree\n",
    "plt.plot(fpr4, tpr4, 'r--', label = 'KNN (AUC=%0.2f)' %np.mean(auc_knn)) #KNN\n",
    "plt.plot(np.arange(0,1.1, 0.1), np.arange(0,1.1, 0.1), label = 'random classification')\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend()\n",
    "plt.figure(figsize=(14, 10),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive bayes\n",
    "auc_nb = {}\n",
    "alphaoptions = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "for val in alphaoptions:\n",
    "    auc_nb[val] = []\n",
    "    \n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    \n",
    "    X_train_fold, X_test_fold = X_train[cols_to_use].iloc[train_index], X_train[cols_to_use].iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    for i in alphaoptions:\n",
    "        \n",
    "        model = BernoulliNB(alpha=i).fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = model.predict_proba(X_test_fold)[:,1]\n",
    "        fpr2, tpr2, thresholds = roc_curve(y_test_fold, y_pred_fold)\n",
    "        auc_nb[i].append(auc(fpr2, tpr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averagenb = []\n",
    "for entry in auc_nb:\n",
    "    averagenb.append(np.mean(auc_nb[entry]))\n",
    "\n",
    "bestalpha = list(auc_nb.keys())[averagenb.index(max(averagenb))]\n",
    "print(bestalpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(alphaoptions, averagenb)\n",
    "plt.title('AUC vs alpha in NB model')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the final model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliNB().fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)[:,1]\n",
    "fpr_final, tpr_final, thresholds = roc_curve(y_test, y_pred)\n",
    "final_auc = auc(fpr2, tpr2)\n",
    "print(final_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the confusion matrix at different thresholds \n",
    "n = y_test.shape[0]\n",
    "thresholds=range(1, 100)\n",
    "thresholds = [x/100 for x in thresholds]\n",
    "accuracy = []\n",
    "recall = []\n",
    "for threshold in thresholds:\n",
    "    y_pred_class = y_pred > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_class).ravel()\n",
    "    #print(\"Threshold: \" + str(threshold))\n",
    "    #print(\"False positive rate: \" + str(fp/(tp+fp)))\n",
    "    #print(\"False negative rate: \" + str(fn/(tn+fn)))    \n",
    "    #print(\"Accuracy: \" + str((tp+tn)/(tp+tn+fp+fn)))\n",
    "    recall.append(tp/(tp+fn))\n",
    "    accuracy.append((tp+tn)/(tp+tn+fp+fn))\n",
    "metrics = pd.DataFrame({\"threshold\": thresholds,\n",
    "                       \"accuracy\": accuracy,\n",
    "                       \"recall\": recall})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(metrics.threshold, metrics.accuracy, label=\"Accuracy\")\n",
    "plt.plot(metrics.threshold, metrics.recall, label=\"Recall Rate\")\n",
    "#plt.axvline(x=0.02)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Threshold Probability for Predicting Resistance\")\n",
    "plt.xlim(0.0, 1)\n",
    "plt.ylim(0.0, 1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(metrics.threshold, metrics.recall)\n",
    "plt.ylabel(\"Recall Rate\")\n",
    "plt.xlabel(\"Threshold Probability for Predicting Resistance\")\n",
    "plt.xlim(0.0, 1)\n",
    "plt.ylim(0.0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = y_pred > 0.02\n",
    "print(metrics.iloc[2, ])\n",
    "conf_mat= confusion_matrix(y_test, y_pred_class)\n",
    "sns.heatmap(conf_mat, fmt='d',square=True, annot=True, cmap=\"YlGnBu\",  xticklabels = ['No Resistance','Resistance'],\n",
    "                                                           yticklabels = ['No Resistance','Resistance'])\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the remaining outcome variables\n",
    "combined_outcomes_test = combined_outcomes.iloc[index_test]\n",
    "combined_outcomes_test[\"y_pred_class\"] = y_pred_class\n",
    "combined_outcomes_test[\"y_true\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_outcomes_test[\"any_weapon_str\"] = \"\"\n",
    "combined_outcomes_test.loc[combined_outcomes_test.any_weapon == 1, \"any_weapon_str\"] = \"Weapon\"\n",
    "combined_outcomes_test.loc[combined_outcomes_test.any_weapon == 0, \"any_weapon_str\"] = \"No Weapon\"\n",
    "ct=pd.crosstab(combined_outcomes_test.loc[combined_outcomes_test.y_true==1, ].y_pred_class, combined_outcomes_test.loc[combined_outcomes_test.y_true==1, ].any_weapon_str).apply(lambda r: r/r.sum(), axis=1)\n",
    "stacked = ct.stack().reset_index().rename(columns={0:'value'})\n",
    "fig = sns.barplot(x=stacked.any_weapon_str, y=stacked.value, hue=stacked.y_pred_class)\n",
    "fig.set(xlabel='Use of a Weapon', ylabel='Percent of Incidents')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.describe().to_csv(\"Output Data/Summary Stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
